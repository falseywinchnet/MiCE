{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ee-cErrP-vb",
    "outputId": "32b94bb2-2bc3-4d4b-ca08-86f41138085b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 1,115,394\n",
      "all the unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 65\n",
      "train has 1,003,854 tokens\n",
      "val has 111,540 tokens\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Prepare the Shakespeare dataset for character-level language modeling.\n",
    "So instead of encoding with GPT-2 BPE tokens, we just map characters to ints.\n",
    "Will save train.bin, val.bin containing the ids, and meta.pkl containing the\n",
    "encoder and decoder and some other related info.\n",
    "\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    base_dir = Path(__file__).parent\n",
    "except NameError:\n",
    "    base_dir = Path(os.getcwd())  # fallback if __file__ is not defined (e.g. in REPL)\n",
    "# download the tiny shakespeare dataset\n",
    "input_file_path = os.path.join(os.path.dirname(base_dir), 'input.txt')\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "print(f\"length of dataset in characters: {len(data):,}\")\n",
    "\n",
    "# get all the unique characters that occur in this text\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print(\"all the unique characters:\", ''.join(chars))\n",
    "print(f\"vocab size: {vocab_size:,}\")\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# create the train and test splits\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "# encode both to integers\n",
    "train_ids = encode(train_data)\n",
    "val_ids = encode(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile(os.path.join(os.path.dirname(base_dir), 'train.bin'))\n",
    "val_ids.tofile(os.path.join(os.path.dirname(base_dir), 'val.bin'))\n",
    "\n",
    "# save the meta information as well, to help us encode/decode later\n",
    "meta = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'itos': itos,\n",
    "    'stoi': stoi,\n",
    "}\n",
    "with open(os.path.join(os.path.dirname(base_dir), 'meta.pkl'), 'wb') as f:\n",
    "    pickle.dump(meta, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "id": "osThn1NuPax0"
   },
   "outputs": [],
   "source": [
    "#copyright joshuah rainstar Joshuah.rainstar@gmail.com 2025\n",
    "#covered under Gratis Public License\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch_mice import (\n",
    "    BatchedICNN,\n",
    "    PositiveLinearHK,\n",
    "    BatchAffineNorm,\n",
    ")\n",
    "import scipy.fftpack\n",
    "from matplotlib import pyplot as plt\n",
    "def dct_basis(L, k):\n",
    "    return torch.tensor(scipy.fftpack.dct(np.eye(L), norm='ortho')[:k], dtype=torch.float32)\n",
    "    \n",
    "class BatchedSingleICNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A minimal wrapper for a BatchedICNN with petals=1, matching VectorHull semantics.\n",
    "    This flattens (B, S, D) → (N=D*B, D), adds a petal dimension, runs the ICNN,\n",
    "    and restores the (B, S, D_out) output.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.icnn = BatchedICNN(in_dim, petals=1, out_dim=out_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, S, D_in)\n",
    "        Returns:\n",
    "            output: (B, S, D_out)\n",
    "        \"\"\"\n",
    "        B, S, D = x.shape\n",
    "        assert D == self.in_dim, f\"Expected last dim {self.in_dim}, got {D}\"\n",
    "        N = B * S\n",
    "\n",
    "        x_flat = x.reshape(N, D)                 # (N, D)\n",
    "        x_proj = x_flat.unsqueeze(0)             # (1, N, D) — single petal\n",
    "\n",
    "        # BatchedICNN forward takes (P, N, D), (N, D) → (N, P, D_out)\n",
    "        out = self.icnn(x_proj, x_flat)          # (N, 1, D_out)\n",
    "        out = out.squeeze(1)                     # (N, D_out)\n",
    "\n",
    "        return out.reshape(B, S, self.out_dim)   # (B, S, D_out)\n",
    "\n",
    "        \n",
    "# === VAE-based Query Generator ===\n",
    "class DisentangledQueryGenerator(nn.Module):\n",
    "    def __init__(self, hash_dim, static_dim, dynamic_dim, out_shape):\n",
    "        super().__init__()\n",
    "        self.out_shape = out_shape  # (L, D)\n",
    "        L, D = out_shape\n",
    "        self.L = L\n",
    "        self.D = D\n",
    "        self.static_dim = static_dim\n",
    "        self.dynamic_dim = dynamic_dim\n",
    "\n",
    "        # z_f ~ q(z_f | h)\n",
    "        self.encoder_f = nn.Sequential(\n",
    "            nn.Linear(hash_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, static_dim * 2)\n",
    "        )\n",
    "\n",
    "        # z_t ~ q(z_t | h), now outputs per-position latents\n",
    "        self.encoder_t = nn.Sequential(\n",
    "            nn.Linear(hash_dim, L * dynamic_dim * 2)\n",
    "        )\n",
    "\n",
    "        # Decoder maps (z_f ⊕ z_t) → Q_t\n",
    "        self.decoder = BatchedSingleICNN(in_dim=static_dim + dynamic_dim, out_dim=D)\n",
    "\n",
    "    def forward(self, h):  # h: (B, hash_dim)\n",
    "        B = h.size(0)\n",
    "        L, D = self.out_shape\n",
    "        D_f = self.static_dim\n",
    "        D_t = self.dynamic_dim\n",
    "\n",
    "        # -- Static latent z_f --\n",
    "        mu_f, logvar_f = self.encoder_f(h).chunk(2, dim=-1)  # (B, D_f)\n",
    "        std_f = torch.exp(0.5 * logvar_f)\n",
    "        eps_f = torch.randn_like(std_f)\n",
    "        z_f = mu_f + eps_f * std_f                            # (B, D_f)\n",
    "\n",
    "        # -- Dynamic latent z_t (per position) --\n",
    "        mu_logvar_t = self.encoder_t(h).view(B, L, 2 * D_t)   # (B, L, 2*D_t)\n",
    "        mu_t, logvar_t = mu_logvar_t.chunk(2, dim=-1)         # (B, L, D_t)\n",
    "        std_t = torch.exp(0.5 * logvar_t)\n",
    "        eps_t = torch.randn_like(std_t)\n",
    "        z_t = mu_t + eps_t * std_t                            # (B, L, D_t)\n",
    "\n",
    "        # -- Expand static z_f across positions --\n",
    "        z_f_exp = z_f.unsqueeze(1).expand(-1, L, -1)          # (B, L, D_f)\n",
    "\n",
    "        # -- Concatenate (B, L, D_f + D_t) --\n",
    "        z_cat = torch.cat([z_f_exp, z_t], dim=-1)\n",
    "\n",
    "        # -- Decode Q: (B, L, D)\n",
    "        Q = self.decoder(z_cat)                               # (B, L, D)\n",
    "\n",
    "        # -- KL divergence with free bits\n",
    "        kl_f = -0.5 * (1 + logvar_f - mu_f.pow(2) - logvar_f.exp())\n",
    "        kl_t = -0.5 * (1 + logvar_t - mu_t.pow(2) - logvar_t.exp())\n",
    "\n",
    "        kl_f = torch.clamp(kl_f, min=0.2).sum(dim=-1).mean()           # scalar\n",
    "        kl_t = torch.clamp(kl_t, min=0.2).sum(dim=-1).mean()           # scalar\n",
    "\n",
    "        return Q, kl_f + kl_t\n",
    "\n",
    "class ConvexHyperQueryGenerator(nn.Module):\n",
    "    def __init__(self, hash_dim, out_shape):\n",
    "        super().__init__()\n",
    "        self.out_shape = out_shape\n",
    "        out_dim = int(torch.prod(torch.tensor(out_shape)))\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            PositiveLinearHK(hash_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            PositiveLinearHK(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, out_dim)  # allows directional freedom\n",
    "        )\n",
    "\n",
    "    def forward(self, h):\n",
    "        Q = self.net(h).view(h.size(0), *self.out_shape)\n",
    "        return Q\n",
    "        \n",
    "# === Hypernetwork-based Key Generator ===\n",
    "class ConvexHyperKeyGenerator(nn.Module):\n",
    "    def __init__(self, hash_dim, out_shape):\n",
    "        super().__init__()\n",
    "        self.out_shape = out_shape\n",
    "        out_dim = int(torch.prod(torch.tensor(out_shape)))\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            PositiveLinearHK(hash_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            PositiveLinearHK(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, out_dim)  # <--- Unconstrained\n",
    "        )\n",
    "    def forward(self, h):\n",
    "        K = self.net(h).view(h.size(0), *self.out_shape)\n",
    "        return K\n",
    "\n",
    "\n",
    "class ConvexICNNHash(nn.Module):\n",
    "    def __init__(self, seq_len, hash_dim):\n",
    "        super().__init__()\n",
    "        self.icnn = BatchedSingleICNN(in_dim=seq_len, out_dim=hash_dim)\n",
    "        self.final_activation = nn.Softplus()\n",
    "\n",
    "    def forward(self, x):  # x: (B, L, D)\n",
    "        x_mean = x.mean(dim=-1)               # (B, L)\n",
    "        x_for_icnn = x_mean.unsqueeze(1)      # (B, 1, L)\n",
    "        h = self.icnn(x_for_icnn).squeeze(1)  # (B, hash_dim)\n",
    "        return  h / (h.norm(dim=-1, keepdim=True) + 1e-6)\n",
    "\n",
    "def normalized_alpha_softplus_attention(Q, K, V, alpha=1.5, tau=0.5, eps=1e-6, log=True):\n",
    "    # 1. Scaled dot product\n",
    "    logits = torch.einsum(\"bqd,bkd->bqk\", Q, K)  # both are (B, L, D)\n",
    "    logits = logits / math.sqrt(Q.size(-1))       # scale by √D\n",
    "  \n",
    "\n",
    "\n",
    "    # 3. α-softplus-like activation (optional smooth ReLU)\n",
    "    # Using F.softplus here instead of raw relu\n",
    "    scores = F.softplus((alpha - 1) * logits - tau) ** (1 / (alpha - 1))\n",
    "    # 4. Normalize manually (row-wise softmax)\n",
    "    weights = scores / (scores.sum(dim=-1, keepdim=True) + eps)\n",
    "    attn_score = weights.sum(dim=2)  # (B, K)\n",
    "\n",
    "    # Normalize each sample separately (min-max per row)\n",
    "    min_vals = attn_score.min(dim=-1, keepdim=True).values\n",
    "    max_vals = attn_score.max(dim=-1, keepdim=True).values\n",
    "    attn_score = (attn_score - min_vals) / (max_vals - min_vals + 1e-6)  # (B, K)\n",
    "    \n",
    "\n",
    "    # 5. Apply attention\n",
    "    return torch.matmul(weights, V),attn_score\n",
    "    \n",
    "class PerceptualAttentionBlock(nn.Module):\n",
    "    def __init__(self, model_dim, hash_dim, latent_dim, seq_len):\n",
    "        super().__init__()\n",
    "        self.hash = ConvexICNNHash(seq_len, hash_dim)\n",
    "        self.query_gen = ConvexHyperQueryGenerator(hash_dim, (seq_len, model_dim))\n",
    "        self.key_gen = ConvexHyperKeyGenerator(hash_dim, (seq_len, model_dim))\n",
    "        self.q_proj = nn.Linear(model_dim, model_dim)\n",
    "        self.k_proj = nn.Linear(model_dim, model_dim)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, L, D) — used as Q input, K input, and V source\n",
    "        Returns:\n",
    "            attention output: (B, L, D)\n",
    "            kl divergence from VAE\n",
    "        \"\"\"\n",
    "        h = self.hash(x)                # (B, hash_dim)\n",
    "        Q = self.query_gen(h)    # (B, L, D)\n",
    "        K = self.key_gen(h)            # (B, L, D)\n",
    "        Q = self.q_proj(Q)\n",
    "        K = self.k_proj(K)\n",
    "        K = F.normalize(K, dim=-1)\n",
    "        Q = F.normalize(Q, dim=-1)  # Unit direction: OK\n",
    "\n",
    "\n",
    "       \n",
    "        attn_out,attn_score = normalized_alpha_softplus_attention(Q, K, x)\n",
    "        return attn_out, 0 ,attn_score\n",
    "\n",
    "\n",
    "        \n",
    "class ConvexBlock(nn.Module):\n",
    "    def __init__(self, model_dim: int, seq_len: int, hash_dim: int, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.attn  = PerceptualAttentionBlock(model_dim, hash_dim, latent_dim, seq_len)\n",
    "        self.norm1 = BatchAffineNorm(model_dim)\n",
    "        self.icnn1 = BatchedSingleICNN(in_dim=model_dim, out_dim=model_dim)\n",
    "        self.norm2 = BatchAffineNorm(model_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, L, dim)\n",
    "        att,kl, attn_score= self.attn(self.norm1(x))\n",
    "        x = x + att       # → (B, L, dim)\n",
    "\n",
    "        # ICNN #1 + norm\n",
    "        x = x + self.icnn1(self.norm2(x))          # → (B, L, dim)\n",
    "\n",
    "        # residual add\n",
    "        return x,kl , attn_score                      # → (B, L, dim)\n",
    "\n",
    "\n",
    "class ConvexLanguageModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        model_dim: int = 128,\n",
    "        seq_len: int = 128,\n",
    "        num_blocks: int = 6,\n",
    "        hash_dim: int = 32,\n",
    "        latent_dim: int = 16,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, model_dim)\n",
    "        self.wpe = nn.Embedding(seq_len, model_dim)\n",
    "        self.num_blocks = num_blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ConvexBlock(model_dim, seq_len, hash_dim, latent_dim)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        self.decoder = BatchedSingleICNN(in_dim=model_dim, out_dim=vocab_size)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids: torch.LongTensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        input_ids: (B, L)\n",
    "        returns   : logits (B, L, vocab_size)\n",
    "        \"\"\"\n",
    "        # -- Embed\n",
    "    \n",
    "        x = self.embedding(input_ids)            # (B, L, E)\n",
    "        B,t,_ = x.shape\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "        pos_emb = self.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = x + pos_emb\n",
    "        # -- Convex blocks\n",
    "        kl = 0.0\n",
    "        attn_scores = []\n",
    "        for block in self.blocks:\n",
    "            x, klt ,attn_score = block(x)                         # (B, L, E)\n",
    "            kl += klt\n",
    "            attn_scores.append(attn_score)\n",
    "\n",
    "        # -- Decode\n",
    "        attn_vis = torch.stack(attn_scores).mean(dim=0)\n",
    "        logits = self.decoder(x)                 # (B, L, V)\n",
    "        return logits,kl/self.num_blocks,attn_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nxAwV6qSUZNY",
    "outputId": "81141f57-0397-4961-b60e-2d6277fd163e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Block is numerically convex (within tol).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def convexity_test(block, B=2, L=4, D=12, tol=1e-6):\n",
    "    block.eval()\n",
    "    # two random inputs\n",
    "    X1 = torch.rand(B, L, D)\n",
    "    X2 = torch.rand(B, L, D)\n",
    "    λ  = 0.3\n",
    "\n",
    "    # mixed input\n",
    "    Xm = λ*X1 + (1-λ)*X2\n",
    "\n",
    "    with torch.no_grad():\n",
    "        f1,_,_ = block(X1)\n",
    "        f2,_,_ = block(X2)\n",
    "        fm,_,_ = block(Xm)\n",
    "\n",
    "    # Convexity: f(λx1+(1−λ)x2) ≤ λ f(x1) + (1−λ) f(x2)\n",
    "    rhs = λ*f1 + (1-λ)*f2\n",
    "    if torch.all(fm <= rhs + tol):\n",
    "        print(\"✅ Block is numerically convex (within tol).\")\n",
    "    else:\n",
    "        diff = (fm - rhs).clamp(min=0)\n",
    "        print(f\"❌ Violation max Δ = {diff.max().item():.3e}\")\n",
    "\n",
    "# Example usage\n",
    "block = ConvexBlock(model_dim=12, seq_len=4, hash_dim=6, latent_dim=24)\n",
    "convexity_test(block)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb427e1d29ad4dfebd8a8f0d8ea5c39f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torch\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "CHAR_WIDTH = 8  # Font size 8 for token rendering\n",
    "CHAR_HEIGHT = 11\n",
    "SEQ_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "LOSS_BAR_HEIGHT = 32\n",
    "EWMA_HEIGHT = 32  # Increased to accommodate large text (previously 32)\n",
    "\n",
    "# Full-resolution framebuffer dimensions\n",
    "container_width = CHAR_WIDTH * SEQ_LEN  # 1024 pixels\n",
    "container_height = CHAR_HEIGHT * BATCH_SIZE  # 176 pixels\n",
    "total_height = container_height + LOSS_BAR_HEIGHT + EWMA_HEIGHT  # Adjusted for larger EWMA\n",
    "\n",
    "# Final scaled-down dimensions\n",
    "scaled_width = container_width   # 512 pixels\n",
    "scaled_height = total_height  # 170 pixels\n",
    "\n",
    "# Initialize framebuffer\n",
    "framebuffer = np.zeros((total_height, container_width, 3), dtype=np.uint8)\n",
    "\n",
    "# EWMA storage\n",
    "ticker_history = np.zeros(SEQ_LEN, dtype=np.float32)  # Stock ticker moving buffer\n",
    "loss_memory = 0.0\n",
    "# Load font\n",
    "try:\n",
    "    font = ImageFont.truetype(\"DejaVuSansMono.ttf\", 8)  # Monospaced font\n",
    "    font_large = ImageFont.truetype(\"DejaVuSansMono.ttf\", 64)  # Large EWMA display\n",
    "except:\n",
    "    font = ImageFont.load_default()\n",
    "    font_large = font\n",
    "\n",
    "# --- Color Mapping Functions ---\n",
    "def get_flame_color(val):\n",
    "    \"\"\"Map a normalized value to a flame-like color.\"\"\"\n",
    "    return np.array([int(val * 255), int(val * 0.5 * 255), 0], dtype=np.uint8)\n",
    "\n",
    "# --- IPython Display Setup ---\n",
    "out = widgets.Output()\n",
    "display(out)\n",
    "\n",
    "def get_dynamic_color(attn_val, loss_val):\n",
    "    \"\"\"\n",
    "    Compute a dynamic color transition between flame orange (uncertain) and phosphor green (confident).\n",
    "\n",
    "    attn_val: Normalized attention value (0 to 1)\n",
    "    loss_val: Normalized loss value (0 to 1, inverted as 1 - loss)\n",
    "\n",
    "    Returns an RGB color as a NumPy array.\n",
    "    colors late in training will often be red. this is suggested to swap out for get_flame_color\n",
    "    but only on fine tuning on new data.\n",
    "    \"\"\"\n",
    "    certainty = 1 - loss_val  # High certainty = low loss\n",
    "\n",
    "    # Define RGB endpoints\n",
    "    orange = np.array([attn_val * 255, attn_val * 0.5 * 255, 0], dtype=np.uint8)   # Uncertain (High Loss)\n",
    "    green = np.array([attn_val * 0.5 * 255, attn_val * 255, attn_val * 0.25 * 255], dtype=np.uint8)  # Confident (Low Loss)\n",
    "\n",
    "    # Interpolate based on certainty (0 = uncertain/orange, 1 = confident/green)\n",
    "    color = (certainty * green) + ((1 - certainty) * orange)\n",
    "\n",
    "    return color.astype(np.uint8)\n",
    "def normalize_rows(x: np.ndarray) -> np.ndarray:\n",
    "    min_val = np.min(x, axis=1, keepdims=True)\n",
    "    max_val = np.max(x, axis=1, keepdims=True)\n",
    "    scale = max_val - min_val\n",
    "    return (x - min_val) / (scale + 1e-16)\n",
    "    \n",
    "# --- Framebuffer Update Function ---\n",
    "def update_framebuffer(attn_weights, token_losses, current_loss, tokens):\n",
    "    token_losses = normalize_rows(token_losses)\n",
    "    attn_weights = normalize_rows(attn_weights)\n",
    "    \"\"\"Render the text grid with coloration based on attn * inverse loss.\"\"\"\n",
    "    global framebuffer, loss_history, ticker_history, loss_memory\n",
    "\n",
    "    # Normalize to [0,1]\n",
    "\n",
    "    # Create image buffer\n",
    "    img = Image.new(\"RGB\", (container_width, total_height), (0, 0, 0))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    # Render text with colored intensity\n",
    "    char_positions = [\n",
    "        (col * CHAR_WIDTH, row * CHAR_HEIGHT + EWMA_HEIGHT + LOSS_BAR_HEIGHT, tokens[row][col])\n",
    "        for row in range(BATCH_SIZE) for col in range(SEQ_LEN)\n",
    "    ]\n",
    "    colors = [\n",
    "        tuple(get_dynamic_color(attn_weights[row, col], token_losses[row, col]))\n",
    "        for row in range(BATCH_SIZE) for col in range(SEQ_LEN)\n",
    "    ]\n",
    "    for (x, y, char), color in zip(char_positions, colors):\n",
    "        draw.text((x, y), char, font=font, fill=color)\n",
    "\n",
    "\n",
    "    etcerta = 0.367879441  # Constant used in update rule\n",
    "    et = 1 - etcerta\n",
    "    update = loss_memory * et + np.minimum(12, np.maximum(current_loss , 0)) * etcerta\n",
    "    loss_memory = loss_memory * et + update * etcerta\n",
    "    # --- EWMA Display (LARGE FONT) ---\n",
    "    ewma = loss_memory\n",
    "    ewma_text = f\"{ewma:.4f}\"\n",
    "    draw.text((container_width-128, 0), ewma_text, font_size=32, fill=(65,255, 125))\n",
    "\n",
    "    # --- Moving Loss Ticker Graph ---\n",
    "    ticker_history = np.roll(ticker_history, -1)  # Shift left\n",
    "    ticker_history[-1] = current_loss  # Insert new loss on the right\n",
    "\n",
    "    # Rescale ticker dynamically like a stock ticker (normalize to min-max range)\n",
    "    min_loss = np.min(ticker_history)\n",
    "    max_loss = np.max(ticker_history)\n",
    "    range_loss = max_loss - min_loss if max_loss != min_loss else 1\n",
    "    normalized_ticker = (ticker_history - min_loss) / range_loss\n",
    "\n",
    "    # Draw ticker graph line\n",
    "    # Optimized drawing loop (fewer function calls)\n",
    "    y_vals = EWMA_HEIGHT + (1 - normalized_ticker) * LOSS_BAR_HEIGHT\n",
    "    x_vals = np.arange(SEQ_LEN) * CHAR_WIDTH\n",
    "    for i in range(SEQ_LEN - 1):\n",
    "        draw.line([(x_vals[i], y_vals[i]), (x_vals[i + 1], y_vals[i + 1])], fill=(0, 255, 255), width=2)\n",
    "\n",
    "    framebuffer = np.array(img)\n",
    "\n",
    "# --- IPython Display Update Function ---\n",
    "def update_display():\n",
    "    \"\"\"Show the framebuffer, scaled down by half using ipywidgets.\"\"\"\n",
    "    img = Image.fromarray(framebuffer)\n",
    "    img_resized = img.resize((scaled_width, scaled_height), Image.LANCZOS)\n",
    "\n",
    "    with out:\n",
    "        clear_output(wait=True)\n",
    "        display(img_resized)\n",
    "\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d98kJVySUojG",
    "outputId": "43544e0b-3f39-4a3e-b964-fca8643fc05d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28115013\n",
      "Number of parameters:  28115013\n",
      "6811745.0\n",
      "1745661.625\n",
      "693612.375\n",
      "322185.3125\n",
      "128809.203125\n",
      "77945.046875\n",
      "42894.390625\n",
      "19881.140625\n",
      "2844.34619140625\n",
      "172.14068603515625\n",
      "12.848677635192871\n",
      "4.09986686706543\n",
      "4.08946418762207\n",
      "4.077268600463867\n",
      "4.04638671875\n",
      "4.022488594055176\n",
      "4.004036903381348\n",
      "3.958981513977051\n",
      "3.9515013694763184\n",
      "3.910064935684204\n",
      "3.901932716369629\n",
      "3.868828058242798\n",
      "3.8374361991882324\n",
      "3.826965808868408\n",
      "3.803696632385254\n",
      "3.759023904800415\n",
      "3.7373483180999756\n",
      "3.7235372066497803\n",
      "3.702165365219116\n",
      "3.7042391300201416\n",
      "3.666679859161377\n",
      "3.610358715057373\n",
      "3.5868782997131348\n",
      "3.5799131393432617\n",
      "3.5938656330108643\n",
      "3.583686113357544\n",
      "3.5562286376953125\n",
      "3.5474319458007812\n",
      "3.525012969970703\n",
      "3.528042793273926\n",
      "3.5587961673736572\n",
      "3.5471444129943848\n",
      "3.465327501296997\n",
      "3.527066946029663\n",
      "3.4418249130249023\n",
      "3.5142877101898193\n",
      "3.4188084602355957\n",
      "3.425403118133545\n",
      "3.4439144134521484\n",
      "3.4973256587982178\n",
      "3.43867564201355\n",
      "3.477679491043091\n",
      "3.448380947113037\n",
      "3.426710844039917\n",
      "3.4328293800354004\n",
      "3.4178924560546875\n",
      "3.414740562438965\n",
      "3.4230265617370605\n",
      "3.416330575942993\n",
      "3.4230518341064453\n",
      "3.3811583518981934\n",
      "3.429905891418457\n",
      "3.371260404586792\n",
      "3.3133251667022705\n",
      "3.3662168979644775\n",
      "3.356804847717285\n",
      "3.370419979095459\n",
      "3.3768310546875\n",
      "3.4031853675842285\n",
      "3.401209831237793\n",
      "3.4105234146118164\n",
      "3.3630361557006836\n",
      "3.372772693634033\n",
      "3.374199151992798\n",
      "3.345432996749878\n",
      "3.325568914413452\n",
      "3.40519118309021\n",
      "3.3336410522460938\n",
      "3.291879177093506\n",
      "3.3301870822906494\n",
      "3.3541646003723145\n",
      "3.3235387802124023\n",
      "3.348224401473999\n",
      "3.354531764984131\n",
      "3.3109893798828125\n",
      "3.270777702331543\n",
      "3.373195171356201\n",
      "3.3769779205322266\n",
      "3.3586268424987793\n",
      "3.391493082046509\n",
      "3.3238961696624756\n",
      "3.283947467803955\n",
      "3.297832489013672\n",
      "3.301570415496826\n",
      "3.283766984939575\n",
      "3.3030335903167725\n",
      "3.3354873657226562\n",
      "3.2549450397491455\n",
      "3.357609987258911\n",
      "3.2590465545654297\n",
      "3.2530481815338135\n",
      "3.384458065032959\n",
      "3.288667917251587\n",
      "3.403034210205078\n",
      "3.305473566055298\n",
      "3.2951807975769043\n",
      "3.284541130065918\n",
      "3.275831699371338\n",
      "3.4057459831237793\n",
      "3.269693613052368\n",
      "3.347374677658081\n",
      "3.2982468605041504\n",
      "3.3298940658569336\n",
      "3.3013267517089844\n",
      "3.275721311569214\n",
      "3.386847496032715\n",
      "3.397916793823242\n",
      "3.3543968200683594\n",
      "3.2894973754882812\n",
      "3.3259942531585693\n",
      "3.423757553100586\n",
      "3.3275070190429688\n",
      "3.3354620933532715\n",
      "3.344923257827759\n",
      "3.3047471046447754\n",
      "3.3102121353149414\n",
      "3.3876659870147705\n",
      "3.338026762008667\n",
      "3.3478548526763916\n",
      "3.297062397003174\n",
      "3.298638343811035\n",
      "3.3272905349731445\n",
      "3.357752561569214\n",
      "3.2691736221313477\n",
      "3.306245803833008\n",
      "3.3666892051696777\n",
      "3.316620349884033\n",
      "3.389836311340332\n",
      "3.387453317642212\n",
      "3.344958782196045\n",
      "3.2416317462921143\n",
      "3.255080223083496\n",
      "3.3228139877319336\n",
      "3.259697914123535\n",
      "3.3930649757385254\n",
      "3.312317371368408\n",
      "3.3056466579437256\n",
      "3.3131470680236816\n",
      "3.3826937675476074\n",
      "3.3062427043914795\n",
      "3.347463607788086\n",
      "3.3383638858795166\n",
      "3.251410961151123\n",
      "3.3088676929473877\n",
      "3.32413649559021\n",
      "3.291494369506836\n",
      "3.323448896408081\n",
      "3.3974695205688477\n",
      "3.3204545974731445\n",
      "3.2732017040252686\n",
      "3.2557449340820312\n",
      "3.2843856811523438\n",
      "3.341310977935791\n",
      "3.2811460494995117\n",
      "3.31935453414917\n",
      "3.3430004119873047\n",
      "3.312763214111328\n",
      "3.2655386924743652\n",
      "3.285850763320923\n",
      "3.3062140941619873\n",
      "3.252925395965576\n",
      "3.2500460147857666\n",
      "3.321223258972168\n",
      "3.291415214538574\n",
      "3.370217800140381\n",
      "3.3235342502593994\n",
      "3.2792372703552246\n",
      "3.2399423122406006\n",
      "3.337061882019043\n",
      "3.333031415939331\n",
      "3.42598295211792\n",
      "3.2346749305725098\n",
      "3.3449866771698\n",
      "3.2718796730041504\n",
      "3.4151928424835205\n",
      "3.328982353210449\n",
      "3.2864105701446533\n",
      "3.3381400108337402\n",
      "3.357226848602295\n",
      "3.299649953842163\n",
      "3.355870008468628\n",
      "3.3758225440979004\n",
      "3.3054702281951904\n",
      "3.2892050743103027\n",
      "3.3105480670928955\n",
      "3.3555567264556885\n",
      "3.3187851905822754\n",
      "3.2688839435577393\n",
      "3.343419075012207\n",
      "3.2494258880615234\n",
      "3.3216071128845215\n",
      "3.340028762817383\n",
      "3.2923781871795654\n",
      "3.2325620651245117\n",
      "3.278103828430176\n",
      "3.2750144004821777\n",
      "3.3828821182250977\n",
      "3.3416762351989746\n",
      "3.3011927604675293\n",
      "3.2997469902038574\n",
      "3.3268306255340576\n",
      "3.289299726486206\n",
      "3.385756492614746\n",
      "3.2796878814697266\n",
      "3.395609140396118\n",
      "3.348903179168701\n",
      "3.320303440093994\n",
      "3.2819926738739014\n",
      "3.2783474922180176\n",
      "3.281766653060913\n",
      "3.320568561553955\n",
      "3.332070827484131\n",
      "3.370262384414673\n",
      "3.322509765625\n",
      "3.2537126541137695\n",
      "3.3304014205932617\n",
      "3.3513474464416504\n",
      "3.2737677097320557\n",
      "3.340420961380005\n",
      "3.2891287803649902\n",
      "3.26556396484375\n",
      "3.400820732116699\n",
      "3.3118622303009033\n",
      "3.3283321857452393\n",
      "3.2669200897216797\n",
      "3.327589511871338\n",
      "3.3504109382629395\n",
      "3.2904255390167236\n",
      "3.261291742324829\n",
      "3.282456398010254\n",
      "3.362645149230957\n",
      "3.280632495880127\n",
      "3.3686041831970215\n",
      "3.368103504180908\n",
      "3.3034682273864746\n",
      "3.348130702972412\n",
      "3.337737798690796\n",
      "3.255685329437256\n",
      "3.3398265838623047\n",
      "3.3489813804626465\n",
      "3.2270383834838867\n",
      "3.375276803970337\n",
      "3.2985801696777344\n",
      "3.419816493988037\n",
      "3.4121336936950684\n",
      "3.289128065109253\n",
      "3.3750927448272705\n",
      "3.295931339263916\n",
      "3.313720226287842\n",
      "3.2655115127563477\n",
      "3.2416014671325684\n",
      "3.374112606048584\n",
      "3.3074440956115723\n",
      "3.340712785720825\n",
      "3.3026747703552246\n",
      "3.255885601043701\n",
      "3.308198928833008\n",
      "3.248793601989746\n",
      "3.354102373123169\n",
      "3.3160886764526367\n",
      "3.4122202396392822\n",
      "3.3670997619628906\n",
      "3.3584139347076416\n",
      "3.33685040473938\n",
      "3.2766785621643066\n",
      "3.280066967010498\n",
      "3.2914018630981445\n",
      "3.2970311641693115\n",
      "3.406968593597412\n",
      "3.339458703994751\n",
      "3.324871301651001\n",
      "3.322840690612793\n",
      "3.3595476150512695\n",
      "3.3345534801483154\n",
      "3.2874562740325928\n",
      "3.2715351581573486\n",
      "3.2396509647369385\n",
      "3.3578667640686035\n",
      "3.333892345428467\n",
      "3.290858745574951\n",
      "3.3274331092834473\n",
      "3.315934658050537\n",
      "3.359792947769165\n",
      "3.266866683959961\n",
      "3.395714282989502\n",
      "3.3074100017547607\n",
      "3.3200817108154297\n",
      "3.281287908554077\n",
      "3.3292386531829834\n",
      "3.3187825679779053\n",
      "3.34407377243042\n",
      "3.2840983867645264\n",
      "3.3082165718078613\n",
      "3.345733165740967\n",
      "3.2697463035583496\n",
      "3.349248170852661\n",
      "3.4112064838409424\n",
      "3.3253183364868164\n",
      "3.293966293334961\n",
      "3.338468313217163\n",
      "3.31158185005188\n",
      "3.3830184936523438\n",
      "3.2938501834869385\n",
      "3.318065643310547\n",
      "3.261744976043701\n",
      "3.3736352920532227\n",
      "3.2730259895324707\n",
      "3.3742408752441406\n",
      "3.345547676086426\n",
      "3.3631739616394043\n",
      "3.3112006187438965\n",
      "3.2415714263916016\n",
      "3.334101915359497\n",
      "3.3331170082092285\n",
      "3.4126100540161133\n",
      "3.362835168838501\n",
      "3.3052263259887695\n",
      "3.434009552001953\n",
      "3.348832607269287\n",
      "3.308912754058838\n",
      "3.455177068710327\n",
      "3.3341879844665527\n",
      "3.3267712593078613\n",
      "3.2702584266662598\n",
      "3.3361053466796875\n",
      "3.3228180408477783\n",
      "3.262399196624756\n",
      "3.2493958473205566\n",
      "3.334238052368164\n",
      "3.312290668487549\n",
      "3.234539031982422\n",
      "3.4157235622406006\n",
      "3.3636837005615234\n",
      "3.333889961242676\n",
      "3.3186511993408203\n",
      "3.325713872909546\n",
      "3.3455734252929688\n",
      "3.2924413681030273\n",
      "3.286935806274414\n",
      "3.300495147705078\n",
      "3.314793348312378\n",
      "3.351797103881836\n",
      "3.3764123916625977\n",
      "3.2807111740112305\n",
      "3.29127836227417\n",
      "3.290111541748047\n",
      "3.347027063369751\n",
      "3.413973331451416\n",
      "3.2727763652801514\n",
      "3.3074772357940674\n",
      "3.3255224227905273\n",
      "3.2753849029541016\n",
      "3.3590006828308105\n",
      "3.209859609603882\n",
      "3.2963647842407227\n",
      "3.247816562652588\n",
      "3.333608388900757\n",
      "3.288717746734619\n",
      "3.2482686042785645\n",
      "3.409822940826416\n",
      "3.321834087371826\n",
      "3.2049784660339355\n",
      "3.307523727416992\n",
      "3.2836666107177734\n",
      "3.2422401905059814\n",
      "3.356114387512207\n",
      "3.261207103729248\n",
      "3.3186416625976562\n",
      "3.3222131729125977\n",
      "3.3243932723999023\n",
      "3.258878469467163\n",
      "3.2570652961730957\n",
      "3.238893985748291\n",
      "3.269108295440674\n",
      "3.261786699295044\n",
      "3.233022689819336\n",
      "3.328591823577881\n",
      "3.3260245323181152\n",
      "3.2901196479797363\n",
      "3.2852718830108643\n",
      "3.2773499488830566\n",
      "3.20320725440979\n",
      "3.2608304023742676\n",
      "3.3606762886047363\n",
      "3.3807408809661865\n",
      "3.2516722679138184\n",
      "3.3177671432495117\n",
      "3.3898162841796875\n",
      "3.3389129638671875\n",
      "3.270040512084961\n",
      "3.272106409072876\n",
      "3.2719433307647705\n",
      "3.2869873046875\n",
      "3.2343337535858154\n",
      "3.3203024864196777\n",
      "3.2975053787231445\n",
      "3.2925868034362793\n",
      "3.3590033054351807\n",
      "3.3480896949768066\n",
      "3.289548873901367\n",
      "3.2944297790527344\n",
      "3.2798800468444824\n",
      "3.2896149158477783\n",
      "3.280492067337036\n",
      "3.280672550201416\n",
      "3.372102737426758\n",
      "3.2901763916015625\n",
      "3.2905759811401367\n",
      "3.3280415534973145\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = \"cpu\"\n",
    "def wolf_update(p: torch.Tensor,\n",
    "                g: torch.Tensor,\n",
    "                state_p: torch.Tensor,\n",
    "                lr: float):\n",
    "    # define your constants here instead of capturing them\n",
    "    etcerta: float = 0.367879441\n",
    "    et:      float = 1.0 - etcerta\n",
    "\n",
    "    # same logic as before\n",
    "    update    = state_p * et + g * etcerta\n",
    "    new_state = state_p * et + update * etcerta\n",
    "    sign_agree = torch.sign(update) * torch.sign(g)\n",
    "    update    = update + (torch.rand_like(update)*2 - 1) * etcerta * update\n",
    "    p_new     = torch.where(sign_agree > 0, p - lr * update, p)\n",
    "    return p_new, new_state\n",
    "\n",
    "class Wolf(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3):\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                self.state[p]['p'] = torch.zeros_like(p)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = closure() if closure is not None else None\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state_p = self.state[p]['p']\n",
    "                p_new, new_state = wolf_update(p.data, p.grad, state_p, lr)\n",
    "                p.data.copy_(p_new)\n",
    "                state_p.copy_(new_state)\n",
    "        return loss\n",
    "\n",
    "# 1) Load data and meta as before\n",
    "data_dir  = os.path.dirname(base_dir)\n",
    "train_ids = np.fromfile(os.path.join(data_dir, 'train.bin'), dtype=np.uint16)\n",
    "val_ids   = np.fromfile(os.path.join(data_dir, 'val.bin'),   dtype=np.uint16)\n",
    "with open(os.path.join(data_dir, 'meta.pkl'), 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "vocab_size = meta['vocab_size']\n",
    "\n",
    "# 2) Compute data‐marginal q[v]\n",
    "counts = np.bincount(train_ids, minlength=vocab_size).astype(float)\n",
    "q = torch.tensor(counts / counts.sum(), dtype=torch.float32, device=device)  # [V]\n",
    "\n",
    "# 3) Dataset + DataLoader\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = torch.from_numpy(data).long()\n",
    "        self.block_size = block_size\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx : idx + self.block_size]\n",
    "        y = self.data[idx + 1 : idx + self.block_size + 1]\n",
    "        return x, y\n",
    "\n",
    "block_size = 128\n",
    "train_loader = DataLoader(CharDataset(train_ids, block_size),\n",
    "                          batch_size=16, shuffle=True, drop_last=True)\n",
    "val_loader   = DataLoader(CharDataset(val_ids,   block_size),\n",
    "                          batch_size=16, shuffle=False, drop_last=True)\n",
    "virgin  = ConvexLanguageModel(vocab_size=65, model_dim = 128,seq_len = 128,num_blocks = 6, hash_dim = 64,latent_dim = 16)\n",
    "print(sum(p.numel() for p in virgin.parameters()))\n",
    "\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in virgin.parameters()))\n",
    "model = virgin.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-2)#or adam, but i prefer the WOLF.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "losses = []\n",
    "beta = 0.01\n",
    "# 6) Train / eval functions\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits , kl, attn_weights = model(xb)\n",
    "        B, T, V = logits.shape\n",
    "        per_token_loss = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            yb.view(-1),\n",
    "            reduction='none'  # This gives the raw loss per token\n",
    "        ).reshape(B,T) # Shape: (B*S,)\n",
    "        loss = per_token_loss.mean() + kl *0.1\n",
    "        loss_cpu = per_token_loss.cpu().detach().numpy()\n",
    "        tokens = [[itos[idx] for idx in seq.tolist()] for seq in yb]\n",
    "        attn_cpu = attn_weights.cpu().detach().numpy()\n",
    "        update_framebuffer(attn_cpu, loss_cpu, loss.item(), tokens)\n",
    "        update_display()\n",
    "\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "    \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "        optimizer.step()\n",
    "        print(loss.item())\n",
    "        total_loss += loss.item()\n",
    "        losses.append(loss.item())\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch():\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for xb, yb in val_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        B, T, V = logits.shape\n",
    "        total_loss += criterion(logits.view(B*T,V),\n",
    "                                yb.view(B*T)).item()\n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "# 7) Run training\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train_loss = train_epoch()\n",
    "    val_loss   = eval_epoch()\n",
    "    print(f\"Epoch {epoch:2d} | train: {train_loss:.4f} | val: {val_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# --- helpers ---------------------------------------------------------\n",
    "def fenchel_decode(logits, tau=1.0, iters=3):\n",
    "    \"\"\"Fenchel‑dual KL‑regularised projection of -logits (energy).\"\"\"\n",
    "    energy = -logits                        # (B,V)\n",
    "    p = torch.full_like(energy, 1.0 / energy.size(-1))  # uniform start\n",
    "    for _ in range(iters):\n",
    "        p = torch.softmax((-energy / tau) + p.log(), dim=-1)\n",
    "    return p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)#or adam, but i prefer the WOLF.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "beta = 0.1\n",
    "# 6) Train / eval functions\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits,kl = model(xb)\n",
    "        B, T, V = logits.shape\n",
    "        p = F.softmax(logits, dim=-1)      # (B, T, V)\n",
    "        # 1) Standard CE\n",
    "        loss = criterion(logits.view(B*T, V),\n",
    "                                yb.view(B*T))        # Forward\n",
    "        loss = loss + beta * kl\n",
    "        print(kl)\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        print(loss.item())\n",
    "        total_loss += loss.item()\n",
    "        losses.append(loss.item())\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch():\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for xb, yb in val_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        B, T, V = logits.shape\n",
    "        total_loss += criterion(logits.view(B*T,V),\n",
    "                                yb.view(B*T)).item()\n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "# 7) Run training\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train_loss = train_epoch()\n",
    "    val_loss   = eval_epoch()\n",
    "    print(f\"Epoch {epoch:2d} | train: {train_loss:.4f} | val: {val_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# --- helpers ---------------------------------------------------------\n",
    "def fenchel_decode(logits, tau=1.0, iters=3):\n",
    "    \"\"\"Fenchel‑dual KL‑regularised projection of -logits (energy).\"\"\"\n",
    "    energy = -logits                        # (B,V)\n",
    "    p = torch.full_like(energy, 1.0 / energy.size(-1))  # uniform start\n",
    "    for _ in range(iters):\n",
    "        p = torch.softmax((-energy / tau) + p.log(), dim=-1)\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
